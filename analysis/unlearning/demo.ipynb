{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlearning Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install [tinyBenchmarks](https://huggingface.co/datasets/tinyBenchmarks/tinyMMLU) For Efficient Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/felipemaiapolo/tinyBenchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate [CrystalChat](https://huggingface.co/LLM360/CrystalChat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-26:01:00:51,145 INFO     [__main__.py:279] Verbosity set to INFO\n",
      "2024-09-26:01:00:52,505 INFO     [__init__.py:491] `group` and `group_alias` keys in TaskConfigs are deprecated and will be removed in v0.4.5 of lm_eval. The new `tag` field will be used to allow for a shortcut to a group of tasks one does not wish to aggregate metrics across. `group`s which aggregate across subtasks must be only defined in a separate group config file, which will be the official way to create groups that support cross-task aggregation as in `mmlu`. Please see the v0.4.4 patch notes and our documentation: https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/new_task_guide.md#advanced-group-configs for more information.\n",
      "2024-09-26:01:00:57,112 INFO     [__main__.py:376] Selected Tasks: ['tinyMMLU', 'wmdp_bio']\n",
      "2024-09-26:01:00:57,118 INFO     [evaluator.py:161] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-09-26:01:00:57,118 INFO     [evaluator.py:198] Initializing hf model, with arguments: {'pretrained': 'LLM360/CrystalChat', 'trust_remote_code': True}\n",
      "2024-09-26:01:00:57,303 INFO     [huggingface.py:130] Using device 'cuda'\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2024-09-26:01:00:57,960 INFO     [huggingface.py:366] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda'}\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:02<00:00,  1.00s/it]\n",
      "2024-09-26:01:01:04,648 WARNING  [task.py:338] [Task: wmdp_bio] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2024-09-26:01:01:04,649 WARNING  [task.py:338] [Task: wmdp_bio] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2024-09-26:01:01:04,681 INFO     [evaluator.py:279] Setting fewshot random generator seed to 1234\n",
      "2024-09-26:01:01:04,681 INFO     [evaluator.py:279] Setting fewshot random generator seed to 1234\n",
      "2024-09-26:01:01:04,681 WARNING  [model.py:422] model.chat_template was called with the chat_template set to False or None. Therefore no chat template will be applied. Make sure this is an intended behavior.\n",
      "2024-09-26:01:01:04,684 INFO     [task.py:428] Building contexts for wmdp_bio on rank 0...\n",
      "100%|██████████████████████████████████████| 1273/1273 [00:01<00:00, 956.92it/s]\n",
      "2024-09-26:01:01:06,047 INFO     [task.py:428] Building contexts for tinyMMLU on rank 0...\n",
      "100%|███████████████████████████████████████| 100/100 [00:00<00:00, 4102.17it/s]\n",
      "2024-09-26:01:01:06,075 INFO     [evaluator.py:485] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|██████| 5492/5492 [00:26<00:00, 209.60it/s]\n",
      "2024-09-26:01:01:39,177 INFO     [evaluation_tracker.py:269] Output path not provided, skipping saving results aggregated\n",
      "hf (pretrained=LLM360/CrystalChat,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 16\n",
      "| Tasks  |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\n",
      "|--------|------:|------|-----:|--------|---|-----:|---|------|\n",
      "|tinyMMLU|      0|none  |     0|acc_norm|↑  |0.4997|±  |   N/A|\n",
      "|wmdp_bio|      1|none  |     0|acc     |↑  |0.5915|±  |0.0138|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm-eval --model hf \\\n",
    "    --model_args pretrained=LLM360/CrystalChat,trust_remote_code=True,dtype=float16 \\\n",
    "    --tasks tinyMMLU,wmdp_bio \\\n",
    "    --batch_size=16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unlearn CrystalChat\n",
    "\n",
    "This commend shows how to use [unlearn.py](unlearn.py) to unlearn [CrystalChat](https://huggingface.co/LLM360/CrystalChat) on biosecurity with [max_entropy](https://github.com/willieneis/Analysis360-mirror/blob/main/analysis/unlearning/methods/training.py#L420) method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.05it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading bio-forget dataset: 100%|█████████████| 100/100 [00:01<00:00, 68.41it/s]\n",
      "Loading wikitext-test dataset: 100%|████████| 100/100 [00:00<00:00, 2224.73it/s]\n",
      "Training: 100%|█| 25/25 [01:20<00:00,  3.22s/it, Total Loss=-8.1836, Retain Loss\n",
      "Log saved to log.json\n",
      "Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "Model saved to ./models/max-entropy_bio-forget_CrystalChat\n"
     ]
    }
   ],
   "source": [
    "!python unlearn.py --model_name_or_path \"LLM360/CrystalChat\" \\\n",
    "                   --unlearn_method \"max-entropy\" \\\n",
    "                   --forget_topic bio-forget \\\n",
    "                   --lr 5e-5 \\\n",
    "                   --random_seed 23597 \\\n",
    "                   --min_len 50 \\\n",
    "                   --max_len 1000 \\\n",
    "                   --max_unlearn_steps 100 \\\n",
    "                   --use_wandb False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Unlearned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-26:01:31:48,246 INFO     [__main__.py:279] Verbosity set to INFO\n",
      "2024-09-26:01:31:49,523 INFO     [__init__.py:491] `group` and `group_alias` keys in TaskConfigs are deprecated and will be removed in v0.4.5 of lm_eval. The new `tag` field will be used to allow for a shortcut to a group of tasks one does not wish to aggregate metrics across. `group`s which aggregate across subtasks must be only defined in a separate group config file, which will be the official way to create groups that support cross-task aggregation as in `mmlu`. Please see the v0.4.4 patch notes and our documentation: https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/new_task_guide.md#advanced-group-configs for more information.\n",
      "2024-09-26:01:31:54,008 INFO     [__main__.py:376] Selected Tasks: ['tinyMMLU', 'wmdp_bio']\n",
      "2024-09-26:01:31:54,012 INFO     [evaluator.py:161] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-09-26:01:31:54,012 INFO     [evaluator.py:198] Initializing hf model, with arguments: {'pretrained': './models/max-entropy_bio-forget_CrystalChat', 'trust_remote_code': True}\n",
      "2024-09-26:01:31:54,201 INFO     [huggingface.py:130] Using device 'cuda'\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2024-09-26:01:31:54,450 INFO     [huggingface.py:366] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda'}\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:01<00:00,  1.53it/s]\n",
      "2024-09-26:01:32:00,680 WARNING  [task.py:338] [Task: wmdp_bio] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2024-09-26:01:32:00,680 WARNING  [task.py:338] [Task: wmdp_bio] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2024-09-26:01:32:00,712 INFO     [evaluator.py:279] Setting fewshot random generator seed to 1234\n",
      "2024-09-26:01:32:00,712 INFO     [evaluator.py:279] Setting fewshot random generator seed to 1234\n",
      "2024-09-26:01:32:00,712 WARNING  [model.py:422] model.chat_template was called with the chat_template set to False or None. Therefore no chat template will be applied. Make sure this is an intended behavior.\n",
      "2024-09-26:01:32:00,714 INFO     [task.py:428] Building contexts for wmdp_bio on rank 0...\n",
      "100%|██████████████████████████████████████| 1273/1273 [00:01<00:00, 960.14it/s]\n",
      "2024-09-26:01:32:02,073 INFO     [task.py:428] Building contexts for tinyMMLU on rank 0...\n",
      "100%|███████████████████████████████████████| 100/100 [00:00<00:00, 4111.74it/s]\n",
      "2024-09-26:01:32:02,102 INFO     [evaluator.py:485] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|██████| 5492/5492 [00:26<00:00, 209.21it/s]\n",
      "2024-09-26:01:32:31,659 WARNING  [huggingface.py:1353] Failed to get model SHA for ./models/max-entropy_bio-forget_CrystalChat at revision main. Error: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './models/max-entropy_bio-forget_CrystalChat'. Use `repo_type` argument if needed.\n",
      "2024-09-26:01:32:34,771 INFO     [evaluation_tracker.py:269] Output path not provided, skipping saving results aggregated\n",
      "hf (pretrained=./models/max-entropy_bio-forget_CrystalChat,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 16\n",
      "| Tasks  |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\n",
      "|--------|------:|------|-----:|--------|---|-----:|---|------|\n",
      "|tinyMMLU|      0|none  |     0|acc_norm|↑  |0.4199|±  |   N/A|\n",
      "|wmdp_bio|      1|none  |     0|acc     |↑  |0.3708|±  |0.0135|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm-eval --model hf\\\n",
    "    --model_args pretrained=./models/max-entropy_bio-forget_CrystalChat,trust_remote_code=True,dtype=float16 \\\n",
    "    --tasks tinyMMLU,wmdp_bio \\\n",
    "    --batch_size=16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlearn360",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
