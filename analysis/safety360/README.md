# Safety360

Welcome to the ``safety360/`` directory! This folder contains various AI safety implementations for [LLM360](https://www.llm360.ai/) models. 

We currently include the following folders:
1. [`bold/`](bold/) provides sentiment analysis with [BOLD](https://arxiv.org/abs/2101.11718) dataset.
2. [`toxic_detection/`](toxic_detection/) measures model's capability to identify toxic text.
3. [`toxigen/`](toxigen/) evaluate model's toxicity on text generation.
4. [`wmdp/`](wmdp/) evaluate model's hazardous knowledge.

