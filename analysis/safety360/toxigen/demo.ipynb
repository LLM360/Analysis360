{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToxiGen Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[single_ckpt_toxigen.py](single_ckpt_toxigen.py) generates model responses using [ToxiGen prompts](https://huggingface.co/datasets/toxigen/toxigen-data/viewer/prompts) and evaluates their toxicity. Please specify the prompts to test via ``--prompt_keys`` arguments. The list of available keys is [here](https://github.com/microsoft/TOXIGEN/tree/main/prompts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command evaluates ``LLM360/AmberChat`` on ``hate_black_1k`` and ``neutral_black_1k``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:02<00:00,  1.40it/s]\n",
      "Toxigen prompt datasets for keys:['hate_black_1k', 'neutral_black_1k'] loaded\n",
      "Generating responses for hate_black_1k: 100%|███| 16/16 [00:43<00:00,  2.74s/it]\n",
      "Saved model responses to ./AmberChat_hate_black_1k_responses.jsonl\n",
      "Evaluating model responses for hate_black_1k: 100%|█| 16/16 [00:46<00:00,  2.91s\n",
      "Toxic rate for AmberChat responses on hate_black_1k: 955/1000=0.9550\n",
      "Generating responses for neutral_black_1k: 100%|█| 16/16 [00:45<00:00,  2.83s/it\n",
      "Saved model responses to ./AmberChat_neutral_black_1k_responses.jsonl\n",
      "Evaluating model responses for neutral_black_1k: 100%|█| 16/16 [00:57<00:00,  3.\n",
      "Toxic rate for AmberChat responses on neutral_black_1k: 34/1000=0.0340\n",
      "Results saved to ./AmberChat_results.jsonl\n",
      "\n",
      "Summary of Toxicity Rates for AmberChat:\n",
      "hate_black_1k: Toxic rate = 0.9550 (955/1000)\n",
      "neutral_black_1k: Toxic rate = 0.0340 (34/1000)\n"
     ]
    }
   ],
   "source": [
    "!python single_ckpt_toxigen.py --model_name LLM360/AmberChat --prompt_keys hate_black_1k,neutral_black_1k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command evaluates ``LLM360/AmberSafe`` on the same set of prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:02<00:00,  1.35it/s]\n",
      "Toxigen prompt datasets for keys:['hate_black_1k', 'neutral_black_1k'] loaded\n",
      "Generating responses for hate_black_1k: 100%|███| 16/16 [00:43<00:00,  2.74s/it]\n",
      "Saved model responses to ./AmberSafe_hate_black_1k_responses.jsonl\n",
      "Evaluating model responses for hate_black_1k: 100%|█| 16/16 [00:48<00:00,  3.04s\n",
      "Toxic rate for AmberSafe responses on hate_black_1k: 815/1000=0.8150\n",
      "Generating responses for neutral_black_1k: 100%|█| 16/16 [00:45<00:00,  2.82s/it\n",
      "Saved model responses to ./AmberSafe_neutral_black_1k_responses.jsonl\n",
      "Evaluating model responses for neutral_black_1k: 100%|█| 16/16 [00:57<00:00,  3.\n",
      "Toxic rate for AmberSafe responses on neutral_black_1k: 202/1000=0.2020\n",
      "Results saved to ./AmberSafe_results.jsonl\n",
      "\n",
      "Summary of Toxicity Rates for AmberSafe:\n",
      "hate_black_1k: Toxic rate = 0.8150 (815/1000)\n",
      "neutral_black_1k: Toxic rate = 0.2020 (202/1000)\n"
     ]
    }
   ],
   "source": [
    "!python single_ckpt_toxigen.py --model_name LLM360/AmberSafe --prompt_keys hate_black_1k,neutral_black_1k"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlearn360",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
