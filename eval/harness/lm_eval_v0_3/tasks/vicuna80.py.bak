from math import exp
from lm_eval.base import rf, Task
from lm_eval.utils import general_detokenize, ARA_DATA_DIR
from lm_eval.metrics import mean
from datasets import load_dataset
import os.path as osp
import json
import time
from functools import partial
from packaging import version

## G42 Translater
import requests
import sys
port_dict = {
 'ar2en': 5000,
 'en2ar': 5001,
 'ar2fr': 5002,
 'ar2ru': 5003,
 'de2en': 5004,
 'en2de': 5005,
 'en2es': 5006,
 'en2fr': 5007,
 'en2it': 5008,
 'en2pt': 5009,
 'en2ru': 5010,
 'en2zh': 5011,
 'es2en': 5012,
 'fr2ar': 5013,
 'fr2en': 5014,
 'it2en': 5015,
 'pt2en': 5016,
 'ru2ar': 5017,
 'ru2en': 5018,
 'zh2en': 5019,
 'ar2de': 5022,
 'de2ar': 5023,
 'ar2es': 5024,
 'es2ar': 5025,
 'ar2it': 5026,
 'it2ar': 5027,
 'ar2pt': 5028,
 'pt2ar': 5029,
 'en2fa': 5020
}

def translate(text, model):
    port = port_dict[model]
    url=f'http://188.116.30.85:{port}/translator/translate'
    #url=f'http://10.111.137.61:8022/translator/translate'
    src = text
    payload = [{"id":model, "src": src}]
    payload = json.dumps(payload)
    headers = {'Content-Type': 'application/json'}
    res = requests.post(url, data=payload, headers=headers)
    try:
        res = json.loads(res.text)
        return '\n'.join([d['tgt'] for d in res])
    except:
        print(res)


# For OpenAI GPT-4 Evaluation
import openai
MAX_API_RETRY = 5
REQ_TIME_GAP = 1
openai.api_key = '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'
def gpt4_eval(sys_prompt, user_prompt: str, max_tokens: int):
    for i in range(MAX_API_RETRY):
        try:
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": sys_prompt},
                    {
                        "role": "user",
                        "content": user_prompt,
                    },
                ],
                temperature=0.2,  # TODO: figure out which temperature is best for evaluation
                max_tokens=max_tokens,
            )
            content = response["choices"][0]["message"]["content"]
            return content
        except Exception as e:
            time.sleep(5)
    print(f"Failed after {MAX_API_RETRY} retries.")
    return "error"

import re
def parse_score(string):
    n1,n2 = -1, -1
    pattern = r"<score1>([0-9]|10)</score1>"
    match = re.search(pattern, string)
    if match:
        n1 = int(match.group(1))

    pattern = r"<score2>([0-9]|10)</score2>"
    match = re.search(pattern, string)
    if match:
        n2 = int(match.group(1))
    return [n1,n2]

prompt_template = {
  "system_prompt": "You are a helpful and precise assistant for checking the quality of the answer in Arabic.",
  "prompt_end": "Suppose the user only speaks Arabic, please evaluate both answers with your justification, and provide an integer score ranging from 0 to 10 after your justifications. When evaluating the answers, you should consider the helpfulness, relevance, accuracy, level of details of the answers. The score for answer 1 should be wrapped by <score1> and </score1>, and the score for answer 2 should be wrapped by <score2> and </score2>.",
  "prompt_template":"<question>\n{question}\n</question>\n\n<answer1>\n{answer_1}\n</answer1>\n\n<answer2>\n{answer_2}\n</answer2>\n\n{prompt_end}"
}


class Vicuna80(Task):
    VERSION = 0
    DATASET_PATH = "vicuna"
    DATASET_NAME = None

    def __init__(self, data_dir=None, cache_dir=None, download_mode=None):
        self.download(ARA_DATA_DIR, cache_dir, download_mode)
        self._training_docs = None
        self._fewshot_docs = None

    def download(self, data_dir=None, cache_dir=None, download_mode=None):
        self.dataset = load_dataset("json", data_files={"test":osp.join(data_dir, "vicuna_questions_with_chatgpt_response.json")})

    def has_training_docs(self):
        return False

    def has_validation_docs(self):
        return False

    def has_test_docs(self):
        return True

    def doc_to_text(self, doc):
        # Use prompt
        return (
            "يوجد أدناه تعليمات تصف مهمة. اكتب ردا يكمل الطلب بشكل مناسب."
            + "\n\n### Instruction: \n"
            + doc["question"]
            + "\n\n ### Response:"
        )

    def should_decontaminate(self):
        return False


    def doc_to_target(self, doc):
        return doc['ChatGPT_response'] # chatgpt output

    def construct_requests(self, doc, ctx):
        """Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """
        continuation = rf.greedy_until(ctx, {"until": None})
        return continuation

    def process_results(self, doc, results, save_all):
        """Take a single document and the LM results and evaluates, returning a
        dict where keys are the names of submetrics and values are the values of
        the metric for that one document

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param results:
            The results of the requests created in construct_requests.
        """

        # Get GPT-4 Evaluation
        doc['Our_model_response'] = results[0]
        print(f"Model output: {doc['model_response']}")
        prompt = prompt_template["prompt_template"].format(
            question=doc['question'],
            answer_1=doc['ChatGPT_response'],
            answer_2=doc['Our_model_response'],
            prompt_end=prompt_template["prompt_end"],
        )
        review = gpt4_eval(prompt_template["system_prompt"], prompt, 2048)
        doc['GPT4_review'] = review
        scores = parse_score(review)

        # Translate
        try:
            tr = translate(doc['Our_model_response'], 'ar2en')
        except:
            tr = ''
        doc['Our_model_response-EN'] = tr

        try:
            tr = translate(review, 'ar2en')
        except:
            tr = ''
        doc['GPT4-review-EN'] = tr

        if save_all:
            result = {"blue":0, 'ChatGPT_score':scores[0], 'Our_model_score':scores[1], "example":doc}

        return result

    def aggregation(self):
        """
        :returns: {str: [float] -> float}
            A dictionary where keys are the names of submetrics and values are
            functions that aggregate a list of metrics
        """
        return {
            "blue":mean,
            "ChatGPT_score":mean,
            "Our_model_score":mean,
        }

    def higher_is_better(self):
        """
        :returns: {str: bool}
            A dictionary where keys are the names of submetrics and values are
            whether a higher value of the submetric is better
        """
        return {
            "blue": True,
            "ChatGPT_score":True,
            "Our_model_score":True,
        }
